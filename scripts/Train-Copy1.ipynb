{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, InputLayer\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "import sys\n",
    "import os\n",
    "pwd = os.getcwd()\n",
    "sys.path.append(\"/root/Gan/jidian/MLexperiments\")\n",
    "sys.path.append(\"/root/Gan/jidian\")\n",
    "sys.path.append(pwd)\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import MLexperiments.classes\n",
    "from MLexperiments.classes import ReadAutoLabeledData\n",
    "import tensorflow as tf\n",
    "import MLexperiments.config.parameters\n",
    "from keras.layers import LSTM, Reshape\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "import datetime\n",
    "import Models\n",
    "import os, errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-f', '/run/user/0/jupyter/kernel-7d2a8c9f-016a-4964-a153-035c3fe620bc.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "#+12h = chinese time zone\n",
    "\n",
    "\n",
    "#\n",
    "tf.flags.DEFINE_integer(\"BATCH_SIZE\",128, \"size\")\n",
    "tf.flags.DEFINE_integer(\"MODEL_TYPE\",0 , \"0: CNN, 1:RNN\")\n",
    "tf.flags.DEFINE_string(\"CHECKPOINTPATH\",'logs/{}'.format(datetime.datetime.fromtimestamp(time()).strftime('%Y-%m-%d %H:%M:%S'))\\\n",
    "                       , \"Path to save the best models\")\n",
    "tf.flags.DEFINE_string(\"MODE\",  1\\\n",
    "                       , \" mode is data source specification;   mode: 1 => 3 human + rain: 1 means high only, 2 high + mid, 3 high+mid+low ,4=>6 man + rain + wind:      4 means high only, 5 high + mid, 6 high+mid+low , 7 => qinshang + factory for testing\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data reading and initialization\n",
    "\n",
    "try:\n",
    "    os.makedirs(FLAGS.CHECKPOINTPATH)\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /root/Gan/jidian/MLexperiments/scripts/utils.py(129)read_data_sets()\n",
      "-> np_input, labelData = np.concatenate((man_input, rain_input)), np.concatenate((man_label, rain_label))\n",
      "(Pdb) c\n",
      "number of each type (balanced):\n",
      "15304\n",
      "total (balanced):\n",
      "30608\n",
      " np_input shape:  (30608, 100, 10) labelData shape:  (30608,)\n",
      "read succesful\n",
      "(1000,)\n",
      "0.0\n",
      "--Return--\n",
      "> <ipython-input-5-eca442341593>(32)<module>()->None\n",
      "-> import pdb;pdb.set_trace()\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "myDataSet = utils.read_data_sets(one_hot=True, test_size = 0.1, validation_size = 0.1, fake_data = False\\\n",
    "                                #todo[][]   , mode = FLAGS.MODE\n",
    "                                )\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) , (X_Ftest, y_Ftest)= (myDataSet.train.images, myDataSet.train.labels), (myDataSet.validation.images, myDataSet.validation.labels), (myDataSet.test.images, myDataSet.test.labels)\n",
    "\n",
    "print(X_train[0].shape)\n",
    "print(y_train[0])\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], MLexperiments.config.parameters.SAMPLE_LEN, MLexperiments.config.parameters.SAMPLE_HEIGHT, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], MLexperiments.config.parameters.SAMPLE_LEN, MLexperiments.config.parameters.SAMPLE_HEIGHT, 1).astype('float32')\n",
    "X_Ftest = X_Ftest.reshape(X_Ftest.shape[0], MLexperiments.config.parameters.SAMPLE_LEN, MLexperiments.config.parameters.SAMPLE_HEIGHT, 1).astype('float32')\n",
    "\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    "\n",
    "\n",
    "def tran_y(y):\n",
    "    y_ohe = np.zeros(MLexperiments.config.parameters.OUTPUTNUM)\n",
    "    \n",
    "    y_ohe[int(y)] = 1 \n",
    "    return y_ohe\n",
    "\n",
    "# def tran_y(y):\n",
    "#     y_ohe = np.zeros(MLexperiments.config.parameters.OUTPUTNUM)\n",
    "#     if y:\n",
    "#         y_ohe[1] = 1\n",
    "#     else:\n",
    "#         y_ohe[0] = 1\n",
    "#     return y_ohe\n",
    "# def tran_y(y):\n",
    "#     return y\n",
    "#import pdb;pdb.set_trace()\n",
    "y_train_ohe = np.array([tran_y(y_train[i]) for i in range(len(y_train))])\n",
    "y_test_ohe = np.array([tran_y(y_test[i]) for i in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 8, 64)         640       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 98, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 96, 6, 128)        73856     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 96, 6, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 96, 6, 32)         4128      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 94, 4, 32)         9248      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 92, 2, 16)         4624      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 92, 2, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2944)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               376960    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 479,891\n",
      "Trainable params: 479,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Models.getMODEL(FLAGS.MODEL_TYPE)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "lossHistory = LossHistory()\n",
    "checkpointer = ModelCheckpoint(filepath = os.path.join(FLAGS.CHECKPOINTPATH, 'best_weights.hdf5'), verbose=1, save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir=FLAGS.CHECKPOINTPATH,histogram_freq=0, batch_size=FLAGS.BATCH_SIZE, write_graph=True, write_grads=False, \\\n",
    "                          write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "model.summary()\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24793 samples, validate on 2754 samples\n",
      "Epoch 1/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.6249 - acc: 0.6443Epoch 00000: val_loss improved from inf to 0.25225, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 102s - loss: 0.6235 - acc: 0.6452 - val_loss: 0.2522 - val_acc: 0.8914\n",
      "Epoch 2/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.8969Epoch 00001: val_loss improved from 0.25225 to 0.15487, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 100s - loss: 0.2392 - acc: 0.8972 - val_loss: 0.1549 - val_acc: 0.9397\n",
      "Epoch 3/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9339Epoch 00002: val_loss improved from 0.15487 to 0.09769, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 100s - loss: 0.1670 - acc: 0.9340 - val_loss: 0.0977 - val_acc: 0.9666\n",
      "Epoch 4/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9620Epoch 00003: val_loss improved from 0.09769 to 0.07929, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 100s - loss: 0.1118 - acc: 0.9620 - val_loss: 0.0793 - val_acc: 0.9735\n",
      "Epoch 5/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9741Epoch 00004: val_loss improved from 0.07929 to 0.06948, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 101s - loss: 0.0815 - acc: 0.9741 - val_loss: 0.0695 - val_acc: 0.9779\n",
      "Epoch 6/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9793Epoch 00005: val_loss improved from 0.06948 to 0.05074, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 100s - loss: 0.0659 - acc: 0.9793 - val_loss: 0.0507 - val_acc: 0.9847\n",
      "Epoch 7/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9824Epoch 00006: val_loss improved from 0.05074 to 0.04186, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 101s - loss: 0.0562 - acc: 0.9824 - val_loss: 0.0419 - val_acc: 0.9869\n",
      "Epoch 8/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9835Epoch 00007: val_loss improved from 0.04186 to 0.03678, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 100s - loss: 0.0525 - acc: 0.9835 - val_loss: 0.0368 - val_acc: 0.9898\n",
      "Epoch 9/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9864Epoch 00008: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0454 - acc: 0.9864 - val_loss: 0.0457 - val_acc: 0.9866\n",
      "Epoch 10/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9881Epoch 00009: val_loss improved from 0.03678 to 0.03237, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 99s - loss: 0.0380 - acc: 0.9881 - val_loss: 0.0324 - val_acc: 0.9887\n",
      "Epoch 11/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9900Epoch 00010: val_loss improved from 0.03237 to 0.02301, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 99s - loss: 0.0340 - acc: 0.9899 - val_loss: 0.0230 - val_acc: 0.9927\n",
      "Epoch 12/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9903Epoch 00011: val_loss improved from 0.02301 to 0.02253, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 99s - loss: 0.0320 - acc: 0.9903 - val_loss: 0.0225 - val_acc: 0.9927\n",
      "Epoch 13/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9915Epoch 00012: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0275 - acc: 0.9916 - val_loss: 0.0273 - val_acc: 0.9913\n",
      "Epoch 14/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9913Epoch 00013: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0266 - acc: 0.9912 - val_loss: 0.0410 - val_acc: 0.9880\n",
      "Epoch 15/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9928Epoch 00014: val_loss improved from 0.02253 to 0.01459, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 100s - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0146 - val_acc: 0.9942\n",
      "Epoch 16/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9924Epoch 00015: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0242 - acc: 0.9925 - val_loss: 0.0190 - val_acc: 0.9931\n",
      "Epoch 17/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9936Epoch 00016: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0186 - acc: 0.9936 - val_loss: 0.0183 - val_acc: 0.9946\n",
      "Epoch 18/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9943Epoch 00017: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0185 - acc: 0.9942 - val_loss: 0.0260 - val_acc: 0.9898\n",
      "Epoch 19/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9955Epoch 00018: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0153 - acc: 0.9955 - val_loss: 0.0572 - val_acc: 0.9833\n",
      "Epoch 20/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9951Epoch 00019: val_loss improved from 0.01459 to 0.00835, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 100s - loss: 0.0140 - acc: 0.9951 - val_loss: 0.0083 - val_acc: 0.9967\n",
      "Epoch 21/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9953Epoch 00020: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0223 - val_acc: 0.9906\n",
      "Epoch 22/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9958Epoch 00021: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0121 - acc: 0.9958 - val_loss: 0.0152 - val_acc: 0.9938\n",
      "Epoch 23/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9956Epoch 00022: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0148 - acc: 0.9956 - val_loss: 0.0137 - val_acc: 0.9942\n",
      "Epoch 24/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9966Epoch 00023: val_loss improved from 0.00835 to 0.00772, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 99s - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0077 - val_acc: 0.9971\n",
      "Epoch 25/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9968Epoch 00024: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0236 - val_acc: 0.9924\n",
      "Epoch 26/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9964Epoch 00025: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0125 - val_acc: 0.9953\n",
      "Epoch 27/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9962Epoch 00026: val_loss improved from 0.00772 to 0.00671, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 100s - loss: 0.0105 - acc: 0.9962 - val_loss: 0.0067 - val_acc: 0.9982\n",
      "Epoch 28/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9974Epoch 00027: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24793/24793 [==============================] - 99s - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0098 - val_acc: 0.9960\n",
      "Epoch 29/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9977Epoch 00028: val_loss improved from 0.00671 to 0.00660, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 99s - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0066 - val_acc: 0.9967\n",
      "Epoch 30/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9981Epoch 00029: val_loss did not improve\n",
      "24793/24793 [==============================] - 101s - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0083 - val_acc: 0.9975\n",
      "Epoch 31/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9973Epoch 00030: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0078 - acc: 0.9972 - val_loss: 0.0480 - val_acc: 0.9851\n",
      "Epoch 32/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9977Epoch 00031: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0071 - acc: 0.9977 - val_loss: 0.0095 - val_acc: 0.9967\n",
      "Epoch 33/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9979Epoch 00032: val_loss improved from 0.00660 to 0.00337, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 100s - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0034 - val_acc: 0.9996\n",
      "Epoch 34/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9948Epoch 00033: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0155 - acc: 0.9948 - val_loss: 0.0054 - val_acc: 0.9982\n",
      "Epoch 35/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9984Epoch 00034: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0056 - val_acc: 0.9975\n",
      "Epoch 36/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9987Epoch 00035: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0133 - val_acc: 0.9956\n",
      "Epoch 37/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9982Epoch 00036: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0058 - acc: 0.9982 - val_loss: 0.0062 - val_acc: 0.9978\n",
      "Epoch 38/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9984Epoch 00037: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0044 - acc: 0.9984 - val_loss: 0.0103 - val_acc: 0.9967\n",
      "Epoch 39/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9984Epoch 00038: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0042 - val_acc: 0.9989\n",
      "Epoch 40/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9989Epoch 00039: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0039 - val_acc: 0.9989\n",
      "Epoch 41/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9990Epoch 00040: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0061 - val_acc: 0.9982\n",
      "Epoch 42/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9989Epoch 00041: val_loss did not improve\n",
      "24793/24793 [==============================] - 99s - loss: 0.0039 - acc: 0.9990 - val_loss: 0.0074 - val_acc: 0.9964\n",
      "Epoch 43/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993Epoch 00042: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0056 - val_acc: 0.9982\n",
      "Epoch 44/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993Epoch 00043: val_loss did not improve\n",
      "24793/24793 [==============================] - 100s - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0053 - val_acc: 0.9978\n",
      "Epoch 45/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9974Epoch 00044: val_loss did not improve\n",
      "24793/24793 [==============================] - 66s - loss: 0.0100 - acc: 0.9975 - val_loss: 0.0056 - val_acc: 0.9982\n",
      "Epoch 46/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993Epoch 00045: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0044 - val_acc: 0.9989\n",
      "Epoch 47/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9989Epoch 00046: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0072 - val_acc: 0.9982\n",
      "Epoch 48/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991Epoch 00047: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0037 - val_acc: 0.9993\n",
      "Epoch 49/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9990Epoch 00048: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0054 - val_acc: 0.9989\n",
      "Epoch 50/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9951Epoch 00049: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0354 - acc: 0.9952 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "Epoch 51/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9979Epoch 00050: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0060 - acc: 0.9979 - val_loss: 0.0054 - val_acc: 0.9982\n",
      "Epoch 52/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9989Epoch 00051: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0100 - val_acc: 0.9967\n",
      "Epoch 53/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9991Epoch 00052: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0047 - val_acc: 0.9985\n",
      "Epoch 54/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9994Epoch 00053: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0035 - val_acc: 0.9989\n",
      "Epoch 55/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995Epoch 00054: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0053 - val_acc: 0.9989\n",
      "Epoch 56/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9989Epoch 00055: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0039 - val_acc: 0.9993\n",
      "Epoch 57/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9991Epoch 00056: val_loss did not improve\n",
      "24793/24793 [==============================] - 55s - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0051 - val_acc: 0.9985\n",
      "Epoch 58/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994Epoch 00057: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0063 - val_acc: 0.9985\n",
      "Epoch 59/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9991Epoch 00058: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0064 - val_acc: 0.9989\n",
      "Epoch 60/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993Epoch 00059: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24793/24793 [==============================] - 56s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0061 - val_acc: 0.9989\n",
      "Epoch 61/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9995Epoch 00060: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0048 - val_acc: 0.9993\n",
      "Epoch 62/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9995Epoch 00061: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0042 - val_acc: 0.9996\n",
      "Epoch 63/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9990Epoch 00062: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0038 - val_acc: 0.9993\n",
      "Epoch 64/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9995Epoch 00063: val_loss improved from 0.00337 to 0.00311, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 56s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0031 - val_acc: 0.9989\n",
      "Epoch 65/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9996Epoch 00064: val_loss improved from 0.00311 to 0.00292, saving model to logs/2017-10-29 23:18:53/best_weights.hdf5\n",
      "24793/24793 [==============================] - 56s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0029 - val_acc: 0.9993\n",
      "Epoch 66/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9995Epoch 00065: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0133 - val_acc: 0.9964\n",
      "Epoch 67/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9996Epoch 00066: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0057 - val_acc: 0.9989\n",
      "Epoch 68/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997Epoch 00067: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0053 - val_acc: 0.9985\n",
      "Epoch 69/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9996Epoch 00068: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0047 - val_acc: 0.9993\n",
      "Epoch 70/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9996Epoch 00069: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0065 - val_acc: 0.9985\n",
      "Epoch 71/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9997Epoch 00070: val_loss did not improve\n",
      "24793/24793 [==============================] - 55s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0048 - val_acc: 0.9993\n",
      "Epoch 72/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9993Epoch 00071: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0040 - val_acc: 0.9993\n",
      "Epoch 73/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 6.8541e-04 - acc: 0.9998Epoch 00072: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 6.8316e-04 - acc: 0.9998 - val_loss: 0.0073 - val_acc: 0.9989\n",
      "Epoch 74/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9994Epoch 00073: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0014 - acc: 0.9994 - val_loss: 0.0043 - val_acc: 0.9993\n",
      "Epoch 75/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9995Epoch 00074: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0047 - val_acc: 0.9996\n",
      "Epoch 76/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9994Epoch 00075: val_loss did not improve\n",
      "24793/24793 [==============================] - 56s - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0052 - val_acc: 0.9996\n",
      "Epoch 77/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9994Epoch 00076: val_loss did not improve\n",
      "24793/24793 [==============================] - 55s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0047 - val_acc: 0.9996\n",
      "Epoch 78/200\n",
      "24704/24793 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9996 "
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"8,7\"\n",
    "\n",
    "history = model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=200, batch_size=FLAGS.BATCH_SIZE, \\\n",
    "                callbacks=[\\\n",
    "                           lossHistory,\\\n",
    "                           checkpointer,\\\n",
    "                           tensorboard\\\n",
    "                          ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(lossHistory.acc)\n",
    "plt.plot(lossHistory.val_acc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('batch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(lossHistory.losses)\n",
    "plt.plot(lossHistory.val_losses)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('batch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "scores = model.evaluate(X_test, y_test_ohe, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for index in range(10):\n",
    "    print(\"expected: \"+str( y_Ftest[index]))\n",
    "    prediction = model.predict(np.expand_dims(X_Ftest[index], axis=0))[0]\n",
    "    print(\"prediction:\" + str(np.argmax(prediction,axis = 0)))\n",
    "    time_series = X_Ftest.reshape(X_Ftest.shape[0], MLexperiments.config.parameters.SAMPLE_LEN, MLexperiments.config.parameters.SAMPLE_HEIGHT) \\\n",
    "        .astype('float32')[index]\n",
    "    # plt.imshow( X_train.reshape(X_train.shape[0], MLexperiments.config.parameters.SAMPLE_LEN, MLexperiments.config.parameters.SAMPLE_HEIGHT).astype('float32')[0]);\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "    plt.plot(time_series.T[0])\n",
    "    plt.plot(time_series.T[1])\n",
    "    plt.plot(time_series.T[2])\n",
    "    plt.plot(time_series.T[3])\n",
    "    plt.plot(time_series.T[4])\n",
    "    plt.plot(time_series.T[5])\n",
    "    plt.plot(time_series.T[6])\n",
    "    plt.plot(time_series.T[7])\n",
    "    plt.plot(time_series.T[8])\n",
    "    plt.plot(time_series.T[9])\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#        vertical_flip=False,\n",
    "#         horizontal_flip=True)\n",
    "\n",
    "# index = 0\n",
    "# time_series = X_test.reshape(X_test.shape[0], MLexperiments.config.parameters.SAMPLE_LEN, MLexperiments.config.parameters.SAMPLE_HEIGHT) \\\n",
    "#         .astype('float32')[index]\n",
    "\n",
    "# # plt.plot(time_series.T[0])\n",
    "# # plt.show()    \n",
    "# # plt.plot(time_series.T[1])\n",
    "# # plt.show()    \n",
    "# # plt.plot(time_series.T[2])\n",
    "# # plt.show()    \n",
    "# # plt.plot(time_series.T[3])\n",
    "# # plt.show()    \n",
    "# # plt.plot(time_series.T[4])\n",
    "# # plt.show()    \n",
    "# # plt.plot(time_series.T[5])\n",
    "# # plt.show()    \n",
    "# # print(\"+++++++++++++++++++++++++++++++++++++++++++++\")    \n",
    "    \n",
    "    \n",
    "# # img = load_img('data/train/cats/cat.0.jpg')  # this is a PIL image\n",
    "# # x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "# # x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "# x = time_series.reshape((1,)+time_series.shape + (1,))\n",
    "# print(\"x.shape  \")\n",
    "# print(x.shape)\n",
    "# # the .flow() command below generates batches of randomly transformed images\n",
    "# # and saves the results to the `preview/` directory\n",
    "# i = 0\n",
    "# for batch in datagen.flow(x, batch_size=1):\n",
    "#     i += 1\n",
    "#     time_series = batch[0]\n",
    "#     time_series = time_series.reshape( MLexperiments.config.parameters.SAMPLE_LEN, MLexperiments.config.parameters.SAMPLE_HEIGHT) \\\n",
    "#         .astype('float32')\n",
    "#     plt.plot(time_series.T[0])\n",
    "#     plt.plot(time_series.T[1])\n",
    "#     plt.plot(time_series.T[2])\n",
    "#     plt.plot(time_series.T[3])\n",
    "#     plt.plot(time_series.T[4])\n",
    "#     plt.plot(time_series.T[5])\n",
    "#     plt.show()\n",
    "# #     plt.plot(time_series.T[0])\n",
    "# #     plt.show()    \n",
    "# #     plt.plot(time_series.T[1])\n",
    "# #     plt.show()    \n",
    "# #     plt.plot(time_series.T[2])\n",
    "# #     plt.show()    \n",
    "# #     plt.plot(time_series.T[3])\n",
    "# #     plt.show()    \n",
    "# #     plt.plot(time_series.T[4])\n",
    "# #     plt.show()    \n",
    "# #     plt.plot(time_series.T[5])\n",
    "# #     plt.show()    \n",
    "# #     print(\"+++++++++++++++++++++++++++++++++++++++++++++\")    \n",
    "    \n",
    "#     if i > 3:\n",
    "#         break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
